!obj:pylearn2.train.Train {
    dataset: &train !obj:pylearn2.datasets.dense_design_matrix.DenseDesignMatrix {
        X: !pkl: 'X_train.pkl',
        y: !pkl: 'y_train.pkl',
        y_labels: 2,
    },

    model: !obj:pylearn2.models.mlp.MLP {
        layers : [
            !obj:pylearn2.models.mlp.RectifiedLinear {
                layer_name: 'h0',
                dim: 500,
                irange: 0.4,
                # Rather than using weight decay, we constrain the norms of the weight vectors
                #max_col_norm: 1.
            },
            !obj:pylearn2.models.mlp.RectifiedLinear {
                layer_name: 'h1',
                dim: 500,
                irange: 0.4,
                # Rather than using weight decay, we constrain the norms of the weight vectors
                #max_col_norm: 1.
            },
            !obj:pylearn2.models.mlp.Softmax {
                layer_name: 'y',
                n_classes: 2,
                irange: .4,
                init_bias_target_marginals: *train
            },
        ],
        nvis: 1225,
    },

    # We train using stochastic gradient descent
    algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {
        batch_size: 1000,

        learning_rate: 1e-1,
        #learning_rule: !obj:pylearn2.training_algorithms.learning_rule.Momentum {
        #    init_momentum: 0.5,
        #},

        # better to use max_col_norm?
        cost: !obj:pylearn2.costs.cost.SumOfCosts {
            costs: [
                !obj:pylearn2.costs.cost.MethodCost {
                    method: 'cost_from_X',
                },
                #!obj:pylearn2.costs.mlp.WeightDecay {
                #    coeffs: [5e-5, 5e-5, 5e-5],
                #},
            ],
        },

        # We monitor how well we're doing during training on a validation set
        monitoring_dataset: {
            'train' : *train,
            'valid' : !obj:pylearn2.datasets.dense_design_matrix.DenseDesignMatrix {
                X: !pkl: 'X_test.pkl',
                y: !pkl: 'y_test.pkl',
                y_labels: 2,
            },
        },

        # We stop after 50 epochs
        termination_criterion: !obj:pylearn2.termination_criteria.EpochCounter {
            max_epochs: 50,
        },
    },

    # We save the model whenever we improve on the validation set classification error
    extensions: [
        !obj:pylearn2.train_extensions.best_params.MonitorBasedSaveBest {
             channel_name: 'valid_y_misclass',
             save_path: "nn_train_best.pkl"
        },
        # http://daemonmaker.blogspot.com/2014/12/monitoring-experiments-in-pylearn2.html
        !obj:pylearn2.train_extensions.live_monitoring.LiveMonitoring {},
        # Not sure what this does...
        #!obj:pylearn2.training_algorithms.sgd.LinearDecayOverEpoch {
        #    start: 5,
        #    saturate: 100,
        #    decay_factor: .01
        #}
    ],

    save_path: "nn_train.pkl",
    save_freq: 1,
}
